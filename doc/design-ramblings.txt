=== need-based-design ===

From a GUI point of view, we start at the highest abstraction possible.
More abstractions may be handy down the line, but let's start with this
one and see where we end up.

The GUI needs:
  * to know all available shares
  * a way to start (/pause/cancel) downloading of shares
  * to know the status of a download
  * (some way to set some settings)
  * a way to advertise new shares
  * (a way to remove/edit/manage an advertised share)

To accomplish this, we have the backend:
  * notify the gui when a new share is available
  * notify the gui when a share becomes unavailable
  * give out share id's the gui can use to start downloads
  * (give out task id's the gui can use to pause/cancel downloads)
  * notify the gui periodically with download status

When we have multiple backends we'll probably need:
  * a 'backend id' as part of the 'share id' and 'task id'
  * a master backend which encapsulates all the other ones,
  * and (de)multiplexes the gui<->backend interaction to the correct real backend

=== ramblings about design ===

Just some thinking about the (distant?) future of UFTT :-p

Some concepts might need to be introduced for extensibility etc:
  * link = some particular way to communicate with other clients
    examples: a connected TCP-connection
              an IPX/UDP broadcast interface
              an IPX/UDP p2p semi-connection
              a multicast connection
  * a single client process can be connected to another client process via multiple links
    examples: both udp and tcp connection over same ethernet cable
              both wired and wireless connection
  * these links will not be 'special' and will be handled just as if they
    had different endpoint clients
  * a method might be needed to mark links as being bundled over the same hardware
      (sharing bandwidth etc)
    , but this is independent of whether the links are for the same endpoint or not
  * would be really neat if that could be autodetected to some usually sensible defaults :-)

Other stuff to think about:
  * multiple links may share the same network backend?
  * broadcast links always available, even without known endpoints
  * links can have more than 1 endpoint (broadcast, multicast)
  * broadcast links can 'discover' endpoints by themselves


=== Info on diagram and stuff ===

* ShareMount
	An array of these structures define the layout of your share.
	It binds a path on the filesystem to a virtual path in your share.
	From this point of view you(everyone) has only 1 share.
	Multiple ShareMounts can be used to populate this share from various sources on your filesystems.
	A ShareMount with an empty 'fs::path' represents an empty directory.
	The virtpath cannot actually exist on the filesystem as reachable from another ShareMount.
	Shorter virtpaths take priority (they can make the longer virtpaths invalid by the above rule).

=== Something about events and actions ===

There are several events that can happen:
  * Receive data from network
  * Gui requests something (dirlisting, download start)
  * download/dirlist finished
  * timeout expired

Threads:
  * GUI thread
  * network thread
  * fileIO thread (hashing?)

Use case 1:
 1. GUI requests client discovery
 2. network sends local discovery broadcast
 3. network receives response(s)
 4. GUI shows clients
 5. GUI requests dir structure (root) of client A
 6. GUI requests dir structure (root) of client B
 7. network sends dir structure requests (local? broadcast?)
 8. receive dir structure of A
 9. show A dir structure in GUI
10. timeout for B
11. re-request B
12. receive dir structure of B
13. show B dir structure in GUI

Information need in use case 1:
- list of UDP interfaces, which serve as client providers (step 2)
- OBJid to links mapping (in case we don't broadcast, (step 5,6,7)
*
- from hash to dirlist or filesystem path(blob)
- list of (tree) OBJid's of which we know their contents (names, hashes, ?size?)
- list of (blob) OBJid's of which we know their contents (from filesystem)
- list of (blob) OBJid's of which we know their contents (in memory, paritally?)
*
fsblobs sha1->modtime+fspath
fstrees sha1->modtime+fspath+(list of sha1's + names)
memtrees sha1->(list of sha1's + names)
memblobs

Jobs/actions/events:
- Client discovery request
-

1. Network thread receives some data
2. It notifies the link to which it belongs

